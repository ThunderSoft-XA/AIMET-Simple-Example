{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型量化\n",
    "## 1. 为何从量化开始？\n",
    "由于，并不熟悉整个量化和压缩的流程，所以我参考了[高通github](https://github.com/quic \"\") 上 的[aimet-model-zoo仓库](https://github.com/quic/aimet-model-zoo \"\") ，这个仓库提供了一些AIMET的使用方法，我便以此入手。\n",
    "\n",
    "同时，我仅仅对Pytorch训练框架较为熟悉，所以仅对仓库中的**zoo_torch目录**下的示例进行了尝试。需要说明的是这个仓库的AIMET版本仍旧停留再**version==1.13.0**,所以部分接口的参数和后续的版本会有略微的差异，但改动不大。\n",
    "\n",
    "仓库中，涉及了训练后量化 (PTQ) 或量化感知训练 (QAT) 等技术，但本次的这个Jupyter笔记本仅会使用到很少的一部分，以便于我们初步了解AIMET的魅力。\n",
    "\n",
    "我们从经典的姿态估计模型入手，开始我们的AIMET之旅。\n",
    "https://github.com/CMU-Perceptual-Computing-Lab/openpose\n",
    "需要说明的是：这个示例使用的是默认的配置文件。仅对权重和激活进行了8bits非对称量化。\n",
    "以下代码，是从zoo_torch/examples/pose_estimation_quanteval.py文件中拆解出来的，以便我们了解使用AIMET的大致过程。\n",
    "\n",
    "\n",
    "# Model quantification\n",
    "## 1. Why start with quantification?\n",
    "Since I am not familiar with the entire quantization and compression process, I refer to [aimet-model-zoo warehouse](https://github. com/quic/aimet-model-zoo \"\"), this repository provides some ways to use AIMET, I will start with it.\n",
    "\n",
    "At the same time, I am only familiar with the Pytorch training framework, so I only tried the examples in the zoo_torch directory in the warehouse. It should be noted that the AIMET version of this warehouse still stays at **version==1.13.0**, so some interface parameters will be slightly different from subsequent versions, but the changes are not big.\n",
    "\n",
    "In the warehouse, techniques such as post-training quantification (PTQ) or quantitative perception training (QAT) are involved, but this Jupyter notebook will only use a small part, so that we can get a preliminary understanding of the charm of AIMET.\n",
    "\n",
    "We start with the classic pose estimation model and start our AIMET journey.\n",
    "https://github.com/CMU-Perceptual-Computing-Lab/openpose\n",
    "It should be noted that this example uses the default configuration file. Only the weight and activation are 8bits asymmetric quantization.\n",
    "The following code is disassembled from the zoo_torch/examples/pose_estimation_quanteval.py file so that we can understand the general process of using AIMET. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "## build CMU pose estimation NN structure\n",
    "\n",
    "class PoseModel(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    CMU pose estimation model.\n",
    "\n",
    "    Based on: \"Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields\":\n",
    "    https://arxiv.org/pdf/1611.08050.pdf\n",
    "\n",
    "    Made lighter and more efficient by Amir (ahabibian@qti.qualcomm.com) in the\n",
    "    Morpheus team.\n",
    "\n",
    "    Some layers of the original commented out to reduce model complexity\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, model_dict, upsample=False):\n",
    "        super(PoseModel, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        self.basemodel = model_dict['block0']\n",
    "        self.pre_stage = model_dict['block_pre_stage']\n",
    "\n",
    "        self.stage1_shared = model_dict['block1_shared']\n",
    "        self.stage1_1 = model_dict['block1_1']\n",
    "        self.stage2_1 = model_dict['block2_1']\n",
    "\n",
    "        self.stage2_shared = model_dict['block2_shared']\n",
    "        self.stage1_2 = model_dict['block1_2']\n",
    "        self.stage2_2 = model_dict['block2_2']\n",
    "\n",
    "    def forward(self, x):\n",
    "        out1_vgg = self.basemodel(x)\n",
    "        out1 = self.pre_stage(out1_vgg)\n",
    "\n",
    "        out1_shared = self.stage1_shared(out1)\n",
    "        out1_1 = self.stage1_1(out1_shared)\n",
    "        out1_2 = self.stage1_2(out1_shared)\n",
    "\n",
    "        out2 = torch.cat([out1_1, out1_2, out1], 1)\n",
    "\n",
    "        out2_shared = self.stage2_shared(out2)\n",
    "        out2_1 = self.stage2_1(out2_shared)\n",
    "        out2_2 = self.stage2_2(out2_shared)\n",
    "\n",
    "        if self.upsample:\n",
    "            # parameters to check for up-sampling: align_corners = True, mode='nearest'\n",
    "            upsampler = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "            out2_1_up = upsampler(out2_1)\n",
    "            out2_2_up = upsampler(out2_2)\n",
    "            return out1_1, out1_2, out2_1, out2_2, out2_1_up, out2_2_up\n",
    "        else:\n",
    "            return out1_1, out1_2, out2_1, out2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model builder class\n",
    "\n",
    "def get_pre_stage_net():\n",
    "    network_dict = {'block_pre_stage': [{'sequential_CPM':\n",
    "                                             [[512, 256, (3, 1), 1, (1, 0), False],\n",
    "                                              [256, 256, (1, 3), 1, (0, 1)]]},\n",
    "                                        {'conv4_4_CPM': [256, 128, 3, 1, 1]}]}\n",
    "    return network_dict\n",
    "\n",
    "\n",
    "def get_shared_network_dict():\n",
    "    network_dict = get_pre_stage_net()\n",
    "    stage_channel = [0, 128, 185, 185, 185, 185, 185]\n",
    "    shared_channel = [0, 112, 128]\n",
    "    sequential4_channel = [0, 32, 48]\n",
    "    for i in range(1, 3):\n",
    "        network_dict['block%d_shared' % i] = \\\n",
    "            [{'sequential1_stage%d_L1' % i:\n",
    "                  [[stage_channel[i], shared_channel[i], (7, 1), 1, (3, 0), False],\n",
    "                   [shared_channel[i], 128, (1, 7), 1, (0, 3)]]},\n",
    "             {'sequential2_stage%d_L1' % i:\n",
    "                  [[128, 112, (7, 1), 1, (3, 0), False],\n",
    "                   [112, 128, (1, 7), 1, (0, 3)]]}]\n",
    "\n",
    "        network_dict['block%d_1' % i] = [{'sequential3_stage%d_L1' % i:\n",
    "                                              [[128, 32, (3, 1), 1, (1, 0), False],\n",
    "                                               [32, 128, (1, 3), 1, (0, 1)]]},\n",
    "                                         {'sequential4_stage%d_L1' % i:\n",
    "                                              [[128, 32, (3, 1), 1, (1, 0), False],\n",
    "                                               [32, 128, (1, 3), 1, (0, 1)]]},\n",
    "                                         {'sequential5_stage%d_L1' % i:\n",
    "                                              [[128, 32, (3, 1), 1, (1, 0), False],\n",
    "                                               [32, 128, (1, 3), 1, (0, 1)]]},\n",
    "                                         {'Mconv6_stage%d_L1' % i: [128, 128, 1, 1, 0]},\n",
    "                                         {'Mconv7_stage%d_L1' % i: [128, 38, 1, 1, 0]}]\n",
    "        network_dict['block%d_2' % i] = [{'sequential3_stage%d_L1' % i:\n",
    "                                              [[128, 32, (3, 1), 1, (1, 0), False],\n",
    "                                               [32, 128, (1, 3), 1, (0, 1)]]},\n",
    "                                         {'sequential4_stage%d_L1' % i:\n",
    "                                              [[128, sequential4_channel[i], (3, 1), 1, (1, 0), False],\n",
    "                                               [sequential4_channel[i], 128, (1, 3), 1, (0, 1)]]},\n",
    "                                         {'sequential5_stage%d_L1' % i:\n",
    "                                              [[128, 48, (3, 1), 1, (1, 0), False],\n",
    "                                               [48, 128, (1, 3), 1, (0, 1)]]},\n",
    "                                         {'Mconv6_stage%d_L2' % i: [128, 128, 1, 1, 0]},\n",
    "                                         {'Mconv7_stage%d_L2' % i: [128, 19, 1, 1, 0]}]\n",
    "    return network_dict\n",
    "\n",
    "\n",
    "def get_model(upsample=False):\n",
    "    block0 = [{'conv0': [3, 32, 3, 1, 1]},\n",
    "              {'sequential1':\n",
    "                   [[32, 16, (3, 1), 1, (1, 0), False],\n",
    "                    [16, 32, (1, 3), 1, (0, 1)]]}, {'pool1_stage1': [2, 2, 0]},\n",
    "              {'sequential2':\n",
    "                   [[32, 32, (3, 1), 1, (1, 0), False],\n",
    "                    [32, 64, (1, 3), 1, (0, 1)]]},\n",
    "              {'sequential3':\n",
    "                   [[64, 32, (3, 1), 1, (1, 0), False],\n",
    "                    [32, 96, (1, 3), 1, (0, 1)]]}, {'pool2_stage1': [2, 2, 0]},\n",
    "              {'sequential4':\n",
    "                   [[96, 80, (3, 1), 1, (1, 0), False],\n",
    "                    [80, 256, (1, 3), 1, (0, 1)]]},\n",
    "              {'sequential5':\n",
    "                   [[256, 80, (3, 1), 1, (1, 0), False],\n",
    "                    [80, 256, (1, 3), 1, (0, 1)]]},\n",
    "              {'sequential6':\n",
    "                   [[256, 48, (3, 1), 1, (1, 0), False],\n",
    "                    [48, 128, (1, 3), 1, (0, 1)]]},\n",
    "              {'sequential7':\n",
    "                   [[128, 48, (3, 1), 1, (1, 0), False],\n",
    "                    [48, 256, (1, 3), 1, (0, 1)]]}, {'pool3_stage1': [2, 2, 0]},\n",
    "              {'sequential8':\n",
    "                   [[256, 96, (3, 1), 1, (1, 0), False],\n",
    "                    [96, 512, (1, 3), 1, (0, 1)]]},\n",
    "              {'sequential9':\n",
    "                   [[512, 192, (3, 1), 1, (1, 0), False],\n",
    "                    [192, 512, (1, 3), 1, (0, 1)]]}]\n",
    "\n",
    "\n",
    "    print(\"defining network with shared weights\")\n",
    "    network_dict = get_shared_network_dict()\n",
    "\n",
    "    def define_base_layers(block, layer_size):\n",
    "        layers = []\n",
    "        for i in range(layer_size):\n",
    "            one_ = block[i]\n",
    "            for k, v in zip(one_.keys(), one_.values()):\n",
    "                if 'pool' in k:\n",
    "                    layers += [nn.MaxPool2d(kernel_size=v[0], stride=v[1], padding=v[2])]\n",
    "                elif 'sequential' in k:\n",
    "                    conv2d_1 = nn.Conv2d(in_channels=v[0][0], out_channels=v[0][1], kernel_size=v[0][2],\n",
    "                                         stride=v[0][3], padding=v[0][4], bias=v[0][5])\n",
    "                    conv2d_2 = nn.Conv2d(in_channels=v[1][0], out_channels=v[1][1], kernel_size=v[1][2],\n",
    "                                         stride=v[1][3], padding=v[1][4])\n",
    "                    sequential = nn.Sequential(conv2d_1, conv2d_2)\n",
    "                    layers += [sequential, nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1], kernel_size=v[2],\n",
    "                                        stride=v[3], padding=v[4])\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "        return layers\n",
    "\n",
    "    def define_stage_layers(cfg_dict):\n",
    "        layers = define_base_layers(cfg_dict, len(cfg_dict) - 1)\n",
    "        one_ = cfg_dict[-1].keys()\n",
    "        k = list(one_)[0]\n",
    "        v = cfg_dict[-1][k]\n",
    "        conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1], kernel_size=v[2], stride=v[3],\n",
    "                           padding=v[4])\n",
    "        layers += [conv2d]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    # create all the layers of the model\n",
    "    base_layers = define_base_layers(block0, len(block0))\n",
    "    pre_stage_layers = define_base_layers(network_dict['block_pre_stage'],\n",
    "                                          len(network_dict['block_pre_stage']))\n",
    "    models = {'block0': nn.Sequential(*base_layers),\n",
    "              'block_pre_stage': nn.Sequential(*pre_stage_layers)}\n",
    "\n",
    "    shared_layers_s1 = define_base_layers(network_dict['block1_shared'],\n",
    "                                          len(network_dict['block1_shared']))\n",
    "    shared_layers_s2 = define_base_layers(network_dict['block2_shared'],\n",
    "                                          len(network_dict['block2_shared']))\n",
    "    models['block1_shared'] = nn.Sequential(*shared_layers_s1)\n",
    "    models['block2_shared'] = nn.Sequential(*shared_layers_s2)\n",
    "\n",
    "    for k, v in zip(network_dict.keys(), network_dict.values()):\n",
    "        if 'shared' not in k and 'pre_stage' not in k:\n",
    "            models[k] = define_stage_layers(v)\n",
    "\n",
    "    model = PoseModel(models, upsample=upsample)\n",
    "    return model\n",
    "\n",
    "class ModelBuilder(object):\n",
    "    def __init__(self, upsample=False):\n",
    "        self.model = None\n",
    "        self.upsample = upsample\n",
    "\n",
    "    def create_model(self):\n",
    "        model = get_model(self.upsample)\n",
    "        self.model = model\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "# Pre-processing and Post-processing,Seems to be \n",
    "\n",
    "def non_maximum_suppression(map, thresh):\n",
    "    map_s = gaussian_filter(map, sigma=3)\n",
    "\n",
    "    map_left = np.zeros(map_s.shape)\n",
    "    map_left[1:, :] = map_s[:-1, :]\n",
    "    map_right = np.zeros(map_s.shape)\n",
    "    map_right[:-1, :] = map_s[1:, :]\n",
    "    map_up = np.zeros(map_s.shape)\n",
    "    map_up[:, 1:] = map_s[:, :-1]\n",
    "    map_down = np.zeros(map_s.shape)\n",
    "    map_down[:, :-1] = map_s[:, 1:]\n",
    "\n",
    "    peaks_binary = np.logical_and.reduce((map_s >= map_left, map_s >= map_right, map_s >= map_up,\n",
    "                                          map_s >= map_down,\n",
    "                                          map_s > thresh))\n",
    "\n",
    "    peaks = zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0])  # note reverse\n",
    "    peaks_with_score = [x + (map[x[1], x[0]],) for x in peaks]\n",
    "\n",
    "    return peaks_with_score\n",
    "\n",
    "\n",
    "def pad_image(img, stride, padding):\n",
    "    h = img.shape[0]\n",
    "    w = img.shape[1]\n",
    "\n",
    "    pad = 4 * [None]\n",
    "    pad[0] = 0  # up\n",
    "    pad[1] = 0  # left\n",
    "    pad[2] = 0 if (h % stride == 0) else stride - (h % stride)  # down\n",
    "    pad[3] = 0 if (w % stride == 0) else stride - (w % stride)  # right\n",
    "\n",
    "    img_padded = img\n",
    "    pad_up = np.tile(img_padded[0:1, :, :] * 0 + padding, (pad[0], 1, 1))\n",
    "    img_padded = np.concatenate((pad_up, img_padded), axis=0)\n",
    "    pad_left = np.tile(img_padded[:, 0:1, :] * 0 + padding, (1, pad[1], 1))\n",
    "    img_padded = np.concatenate((pad_left, img_padded), axis=1)\n",
    "    pad_down = np.tile(img_padded[-2:-1, :, :] * 0 + padding, (pad[2], 1, 1))\n",
    "    img_padded = np.concatenate((img_padded, pad_down), axis=0)\n",
    "    pad_right = np.tile(img_padded[:, -2:-1, :] * 0 + padding, (1, pad[3], 1))\n",
    "    img_padded = np.concatenate((img_padded, pad_right), axis=1)\n",
    "\n",
    "    return img_padded, pad\n",
    "\n",
    "\n",
    "def encode_input(image, scale, stride, padding):\n",
    "    image_scaled = cv2.resize(image, (0, 0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
    "    image_scaled_padded, pad = pad_image(image_scaled, stride, padding)\n",
    "\n",
    "    return image_scaled_padded, pad\n",
    "\n",
    "\n",
    "def decode_output(data, stride, padding, input_shape, image_shape):\n",
    "    output = np.transpose(np.squeeze(data), (1, 2, 0))\n",
    "    output = cv2.resize(output, (0, 0), fx=stride, fy=stride, interpolation=cv2.INTER_CUBIC)\n",
    "    output = output[:input_shape[0] - padding[2], :input_shape[1] - padding[3], :]\n",
    "    output = cv2.resize(output, (image_shape[1], image_shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def preprocess(image, transforms):\n",
    "    mean_bgr = [34.282957728666474, 32.441979567868017, 24.339757511312481]\n",
    "\n",
    "    image = image.astype(np.float32)\n",
    "\n",
    "    if 'bgr' in transforms:\n",
    "        if image.shape[0] == 3:\n",
    "            image = image[::-1, :, :]\n",
    "        elif image.shape[2] == 3:\n",
    "            image = image[:, :, ::-1]\n",
    "\n",
    "    if 'tr' in transforms:\n",
    "        image = image.transpose((2, 0, 1))\n",
    "\n",
    "    if 'mean' in transforms:\n",
    "        image[0, :, :] -= mean_bgr[0]\n",
    "        image[1, :, :] -= mean_bgr[1]\n",
    "        image[2, :, :] -= mean_bgr[2]\n",
    "\n",
    "    if 'addchannel' in transforms:\n",
    "        image = image[np.newaxis, :, :, :]\n",
    "\n",
    "    if 'normalize' in transforms:\n",
    "        image = image / 256 - 0.5\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def run_model(model, image, fast=False):\n",
    "    scale_search = [1.]\n",
    "    crop = 368\n",
    "    stride = 8\n",
    "    padValue = 128\n",
    "\n",
    "    if fast:\n",
    "        scales = scale_search\n",
    "    else:\n",
    "        scales = [x * crop / image.shape[0] for x in scale_search]\n",
    "\n",
    "    heatmaps, pafs = [], []\n",
    "    for scale in scales:\n",
    "        if fast:\n",
    "            horiz = image.shape[0] < image.shape[1]\n",
    "            sz = (496, 384) if horiz else (384, 496)\n",
    "            image_encoded = cv2.resize(image, dsize=(int(sz[0] * scale), int(sz[1] * scale)))\n",
    "        else:\n",
    "            image_encoded, pad = encode_input(image, scale, stride,\n",
    "                                                         padValue)\n",
    "        image_encoded_ = preprocess(image_encoded,\n",
    "                                                      ['addchannel', 'normalize', 'bgr'])\n",
    "        image_encoded_ = np.transpose(image_encoded_, (0, 3, 1, 2))\n",
    "        with torch.no_grad():\n",
    "            input_image = torch.FloatTensor(torch.from_numpy(image_encoded_).float())\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                input_image = input_image.to(device='cuda')\n",
    "            output = model(input_image)\n",
    "        paf = output[2].cpu().data.numpy().transpose((0, 2, 3, 1))\n",
    "        heatmap = output[3].cpu().data.numpy().transpose((0, 2, 3, 1))\n",
    "        if fast:\n",
    "            paf = cv2.resize(paf[0], (image.shape[1], image.shape[0]))\n",
    "            heatmap = cv2.resize(heatmap[0], dsize=(image.shape[1], image.shape[0]))\n",
    "        else:\n",
    "            # paf = paf.transpose((0, 3, 1, 2))\n",
    "            # heatmap = heatmap.transpose((0, 3, 1, 2))\n",
    "            paf = decode_output(paf, stride, pad, image_encoded.shape,\n",
    "                                           image.shape)\n",
    "            heatmap = decode_output(heatmap, stride, pad, image_encoded.shape,\n",
    "                                               image.shape)\n",
    "\n",
    "        pafs.append(paf)\n",
    "        heatmaps.append(heatmap)\n",
    "\n",
    "    return np.asarray(heatmaps).mean(axis=0), np.asarray(pafs).mean(axis=0)\n",
    "\n",
    "\n",
    "def get_keypoints(heatmap):\n",
    "    thre1 = 0.1\n",
    "    keypoints_all = []\n",
    "    keypoints_cnt = 0\n",
    "    for part in range(19 - 1):\n",
    "        keypoints = non_maximum_suppression(heatmap[:, :, part], thre1)\n",
    "        id = range(keypoints_cnt, keypoints_cnt + len(keypoints))\n",
    "        keypoints = [keypoints[i] + (id[i],) for i in range(len(id))]\n",
    "        keypoints_all.append(keypoints)\n",
    "        keypoints_cnt += len(keypoints)\n",
    "    return keypoints_all\n",
    "\n",
    "\n",
    "def get_limb_consistency(paf, start_keypoint, end_keypoint, image_h, div_num=10):\n",
    "    vec_key = np.subtract(end_keypoint[:2], start_keypoint[:2])\n",
    "    vec_key_norm = math.sqrt(vec_key[0] * vec_key[0] + vec_key[1] * vec_key[1])\n",
    "    if vec_key_norm == 0:\n",
    "        vec_key_norm = 1\n",
    "    vec_key = np.divide(vec_key, vec_key_norm)\n",
    "\n",
    "    vec_paf = list(zip(np.linspace(start_keypoint[0], end_keypoint[0], num=div_num).astype(int),\n",
    "                       np.linspace(start_keypoint[1], end_keypoint[1], num=div_num).astype(int)))\n",
    "\n",
    "    vec_paf_x = np.array([paf[vec_paf[k][1], vec_paf[k][0], 0] for k in range(div_num)])\n",
    "    vec_paf_y = np.array([paf[vec_paf[k][1], vec_paf[k][0], 1] for k in range(div_num)])\n",
    "\n",
    "    # To see how well the direction of the prediction over the line connecting the limbs aligns\n",
    "    # with the vec_key we compute the integral of the dot product of the \"affinity vector at point\n",
    "    # 'u' on the line\" and the \"vec_key\".\n",
    "    # In discrete form, this integral is done as below:\n",
    "    vec_sims = np.multiply(vec_paf_x, vec_key[0]) + np.multiply(vec_paf_y, vec_key[1])\n",
    "\n",
    "    # this is just a heuristic approach to punish very long predicted limbs\n",
    "    vec_sims_prior = vec_sims.mean() + min(0.5 * image_h / vec_key_norm - 1, 0)\n",
    "\n",
    "    return vec_sims, vec_sims_prior\n",
    "\n",
    "\n",
    "def connect_keypoints(image_shape, keypoints, paf, limbs, limbsInds):\n",
    "    thre2 = 0.05\n",
    "    connections = []\n",
    "    small_limb_list = [1, 15, 16, 17, 18]\n",
    "    for k in range(len(limbsInds)):\n",
    "        paf_limb = paf[:, :, limbsInds[k]]\n",
    "        limb_strs = keypoints[limbs[k][0]]\n",
    "        limb_ends = keypoints[limbs[k][1]]\n",
    "\n",
    "        if len(limb_strs) != 0 and len(limb_ends) != 0:\n",
    "            cands = []\n",
    "            for i, limb_str in enumerate(limb_strs):\n",
    "                for j, limb_end in enumerate(limb_ends):\n",
    "                    # for each potential pair of keypoints which can have a limb in between we\n",
    "                    # measure a score using the get_limb_consistency function\n",
    "                    if limbs[k][0] in small_limb_list or limbs[k][1] in small_limb_list:\n",
    "                        sims, sims_p = get_limb_consistency(paf_limb, limb_str, limb_end,\n",
    "                                                            image_shape[0], div_num=10)\n",
    "                    else:\n",
    "                        sims, sims_p = get_limb_consistency(paf_limb, limb_str, limb_end,\n",
    "                                                            image_shape[0], div_num=10)\n",
    "                    if len(np.where(sims > thre2)[0]) > int(0.80 * len(sims)) and sims_p > 0:\n",
    "                        cands.append([i, j, sims_p])\n",
    "            cands = sorted(cands, key=lambda x: x[2], reverse=True)\n",
    "            connection = np.zeros((0, 3))\n",
    "            visited_strs, visited_ends = [], []\n",
    "            for cand in cands:\n",
    "                i, j, s = cand\n",
    "                if i not in visited_strs and j not in visited_ends:\n",
    "                    connection = np.vstack([connection, [limb_strs[i][3], limb_ends[j][3], s]])\n",
    "                    visited_strs.append(i)\n",
    "                    visited_ends.append(j)\n",
    "\n",
    "                    if len(connection) >= min(len(limb_strs), len(limb_ends)):\n",
    "                        break\n",
    "            connections.append(connection)\n",
    "        else:\n",
    "            connections.append([])\n",
    "    return connections\n",
    "\n",
    "\n",
    "def create_skeletons(keypoints, connections, limbs):\n",
    "    # last number in each row is the total parts number of that person\n",
    "    # the second last number in each row is the score of the overall configuration\n",
    "    skeletons = -1 * np.ones((0, 20))\n",
    "    keypoints_flatten = np.array([item for sublist in keypoints for item in sublist])\n",
    "\n",
    "    for k in range(len(limbs)):\n",
    "        if len(connections[k]) > 0:\n",
    "            detected_str = connections[k][:, 0]\n",
    "            detected_end = connections[k][:, 1]\n",
    "            limb_str, limb_end = np.array(limbs[k])\n",
    "\n",
    "            for i in range(len(connections[k])):\n",
    "                found = 0\n",
    "                subset_idx = [-1, -1]\n",
    "                for j in range(len(skeletons)):\n",
    "                    if skeletons[j][limb_str] == detected_str[i] or \\\n",
    "                            skeletons[j][limb_end] == detected_end[i]:\n",
    "                        subset_idx[found] = j\n",
    "                        found += 1\n",
    "\n",
    "                if found == 1:\n",
    "                    j = subset_idx[0]\n",
    "                    if skeletons[j][limb_end] != detected_end[i]:\n",
    "                        skeletons[j][limb_end] = detected_end[i]\n",
    "                        skeletons[j][-1] += 1\n",
    "                        skeletons[j][-2] += keypoints_flatten[detected_end[i].astype(int), 2] + \\\n",
    "                                            connections[k][i][2]\n",
    "                elif found == 2:  # if found 2 and disjoint, merge them\n",
    "                    j1, j2 = subset_idx\n",
    "\n",
    "                    membership = ((skeletons[j1] >= 0).astype(int) +\n",
    "                                  (skeletons[j2] >= 0).astype(int))[:-2]\n",
    "                    if len(np.nonzero(membership == 2)[0]) == 0:  # merge\n",
    "                        skeletons[j1][:-2] += (skeletons[j2][:-2] + 1)\n",
    "                        skeletons[j1][-2:] += skeletons[j2][-2:]\n",
    "                        skeletons[j1][-2] += connections[k][i][2]\n",
    "                        skeletons = np.delete(skeletons, j2, 0)\n",
    "                    else:  # as like found == 1\n",
    "                        skeletons[j1][limb_end] = detected_end[i]\n",
    "                        skeletons[j1][-1] += 1\n",
    "                        skeletons[j1][-2] += keypoints_flatten[detected_end[i].astype(int), 2] + \\\n",
    "                                             connections[k][i][2]\n",
    "\n",
    "                # if find no partA in the subset, create a new subset\n",
    "                elif not found and k < 17:\n",
    "                    row = -1 * np.ones(20)\n",
    "                    row[limb_str] = detected_str[i]\n",
    "                    row[limb_end] = detected_end[i]\n",
    "                    row[-1] = 2\n",
    "                    row[-2] = sum(keypoints_flatten[connections[k][i, :2].astype(int), 2]) + \\\n",
    "                              connections[k][i][2]\n",
    "                    skeletons = np.vstack([skeletons, row])\n",
    "\n",
    "    # delete some rows of subset which has few parts occur\n",
    "    deleteIdx = []\n",
    "    for i in range(len(skeletons)):\n",
    "        if skeletons[i][-1] < 4 or skeletons[i][-2] / skeletons[i][-1] < 0.4:\n",
    "            deleteIdx.append(i)\n",
    "    skeletons = np.delete(skeletons, deleteIdx, axis=0)\n",
    "    return {'keypoints': skeletons[:, :18], 'scores': skeletons[:, 18]}\n",
    "\n",
    "\n",
    "def estimate_pose(image_shape, heatmap, paf):\n",
    "    # limbs as pair of keypoints: [start_keypoint, end_keypoint] keypoints index to heatmap matrix\n",
    "    limbs = [[1, 2], [1, 5], [2, 3], [3, 4], [5, 6], [6, 7], [1, 8], [8, 9], [9, 10], [1, 11],\n",
    "             [11, 12], [12, 13],\n",
    "             [1, 0], [0, 14], [14, 16], [0, 15], [15, 17], [2, 16], [5, 17]]\n",
    "    # index where each limb stands in paf matrix. Two consecutive indices for x and y component\n",
    "    # of paf\n",
    "    limbsInd = [[12, 13], [20, 21], [14, 15], [16, 17], [22, 23], [24, 25], [0, 1], [2, 3], [4, 5],\n",
    "                [6, 7], [8, 9],\n",
    "                [10, 11], [28, 29], [30, 31], [34, 35], [32, 33], [36, 37], [18, 19], [26, 27]]\n",
    "\n",
    "    # Computing the keypoints using non-max-suppression\n",
    "    keypoints = get_keypoints(heatmap)\n",
    "\n",
    "    # Computing which pairs of joints should be connected based on the paf.\n",
    "    connections = connect_keypoints(image_shape, keypoints, paf, limbs, limbsInd)\n",
    "\n",
    "    skeletons = create_skeletons(keypoints, connections, limbs)\n",
    "\n",
    "    return skeletons, np.array([item for sublist in keypoints for item in sublist])\n",
    "\n",
    "\n",
    "def parse_results(skeletons, points):\n",
    "    coco_indices = [0, -1, 6, 8, 10, 5, 7, 9, 12, 14, 16, 11, 13, 15, 2, 1, 4, 3]\n",
    "\n",
    "    skeletons_out, scores = [], []\n",
    "    for score, keypoints in zip(skeletons['scores'], skeletons['keypoints']):\n",
    "        skeleton = []\n",
    "        for p in range(len(keypoints)):\n",
    "            if p == 1:\n",
    "                continue\n",
    "            ind = int(keypoints[p])\n",
    "            if ind >= 0:\n",
    "                point = {'x': points[ind, 0], 'y': points[ind, 1], 'score': points[ind, 2],\n",
    "                         'id': coco_indices[p]}\n",
    "                skeleton.append(point)\n",
    "\n",
    "        skeletons_out.append(skeleton)\n",
    "        scores.append(score)\n",
    "    return {'skeletons': skeletons_out, 'scores': scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "# builder Dataloader based on COCO2014\n",
    "\n",
    "class COCOWrapper:\n",
    "    def __init__(self, coco_path, num_imgs=None):\n",
    "        self.coco_path = coco_path\n",
    "        self.num_imgs = num_imgs\n",
    "        # sys.path.append(self.coco_apth + \"codes/PythonAPI\")\n",
    "\n",
    "    def get_images(self):\n",
    "        imgs = self.cocoGT.imgs.values()\n",
    "\n",
    "        image_ids = sorted(map(lambda x: x['id'], self.cocoGT.imgs.values()))\n",
    "        if self.num_imgs:\n",
    "            image_ids = image_ids[:self.num_imgs]\n",
    "        imgs = list(filter(lambda x: x['id'] in image_ids, imgs))\n",
    "\n",
    "        return imgs\n",
    "\n",
    "    def evaluate_json(self, obj):\n",
    "        # initialize COCO detections api\n",
    "        cocoDT = self.cocoGT.loadRes(obj)\n",
    "\n",
    "        imgIds = sorted(self.cocoGT.getImgIds())\n",
    "        if self.num_imgs:\n",
    "            imgIds = imgIds[:self.num_imgs]\n",
    "\n",
    "        # running evaluation\n",
    "        cocoEval = COCOeval(self.cocoGT, cocoDT, 'keypoints')\n",
    "        cocoEval.params.imgIds = imgIds\n",
    "        cocoEval.evaluate()\n",
    "        cocoEval.accumulate()\n",
    "        cocoEval.summarize()\n",
    "        return cocoEval.stats[0::5]\n",
    "\n",
    "    def get_results_json(self, results, imgs):\n",
    "        results_obj = []\n",
    "        for img, result in list(zip(imgs, results)):\n",
    "            for score, skeleton in list(zip(result['scores'], result['skeletons'])):\n",
    "                obj = {'image_id': img['id'], 'category_id': 1, 'keypoints': np.zeros(shape=(3, 17))}\n",
    "\n",
    "                for keypoint in skeleton:\n",
    "                    obj['keypoints'][0, keypoint['id']] = keypoint['x'] - 0.5\n",
    "                    obj['keypoints'][1, keypoint['id']] = keypoint['y'] - 0.5\n",
    "                    obj['keypoints'][2, keypoint['id']] = 1\n",
    "                obj['keypoints'] = list(np.reshape(obj['keypoints'], newshape=(51,), order='F'))\n",
    "                obj['score'] = score / len(skeleton)\n",
    "\n",
    "                results_obj.append(obj)\n",
    "\n",
    "        return results_obj\n",
    "\n",
    "    @property\n",
    "    def cocoGT(self):\n",
    "        annType = 'keypoints'\n",
    "        prefix = 'person_keypoints'\n",
    "        print('Initializing demo for *%s* results.' % (annType))\n",
    "\n",
    "        # initialize COCO ground truth api\n",
    "        dataType = 'val2014'\n",
    "        annFile = os.path.join(self.coco_path, 'annotations/%s_%s.json' % (prefix, dataType))\n",
    "        cocoGT = COCO(annFile)\n",
    "\n",
    "        if not cocoGT:\n",
    "            raise AttributeError('COCO ground truth demo failed to initialize!')\n",
    "\n",
    "        return cocoGT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from functools import partial\n",
    "from aimet_torch import quantsim\n",
    "\n",
    "## model evaluation func\n",
    "def evaluate_model(model,\n",
    "                   coco_path,\n",
    "                   num_imgs=None,\n",
    "                   fast=True):\n",
    "    coco = COCOWrapper(coco_path, num_imgs)\n",
    "\n",
    "    results = []\n",
    "    image_path = os.path.join(coco.coco_path, 'images/val2014/')\n",
    "    imgs = coco.get_images()\n",
    "    print(\"Running extended evaluation on the validation set\")\n",
    "    for i, img in tqdm(enumerate(imgs)):\n",
    "        image = cv2.imread(image_path + img['file_name'])  # B,G,R order\n",
    "\n",
    "        heatmap, paf = run_model(model, image, fast)\n",
    "\n",
    "        skeletons, keypoints = estimate_pose(image.shape, heatmap, paf)\n",
    "        results.append(parse_results(skeletons, keypoints))\n",
    "\n",
    "    try:\n",
    "        ans = coco.evaluate_json(coco.get_results_json(results, imgs))\n",
    "        return ans\n",
    "    except:\n",
    "        return [0, 0]\n",
    "\n",
    "def pose_estimation_quanteval(args):\n",
    "    # load the model checkpoint from meta\n",
    "    model_builder = ModelBuilder()\n",
    "    model_builder.create_model()\n",
    "    model = model_builder.model\n",
    "\n",
    "    state_dict = torch.load(args.model_dir)\n",
    "    state = model.state_dict()\n",
    "    state.update(state_dict)\n",
    "\n",
    "    model.load_state_dict(state)\n",
    "\n",
    "    # create quantsim object which inserts quant ops between layers\n",
    "    #The parameters of quantsim.QuantizationSimModel()  function 1.13 version and 1.16.2 version are different \n",
    "    sim = quantsim.QuantizationSimModel(model,\n",
    "                                        dummy_input=torch.Tensor(1, 3, 128, 128),\n",
    "                                        quant_scheme=args.quant_scheme)\n",
    "\n",
    "    evaluate = partial(evaluate_model,\n",
    "                       num_imgs=500\n",
    "                       )\n",
    "    sim.compute_encodings(evaluate, args.coco_path)\n",
    "\n",
    "    eval_num = evaluate_model(sim.model,\n",
    "                              args.coco_path,\n",
    "                              num_imgs=500\n",
    "                              )\n",
    "    print(f'The [mAP, mAR] results are: {eval_num}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(prog='pose_estimation_quanteval',\n",
    "                                     description='Evaluate the post quantized SRGAN model')\n",
    "\n",
    "    parser.add_argument('model_dir',\n",
    "                        help='The location where the the .pth file is saved,'\n",
    "                             'the .pth contains model weights',\n",
    "                        type=str)\n",
    "    parser.add_argument('coco_path',\n",
    "                        help='The location coco images and annotations are saved. '\n",
    "                             'It assumes a folder structure containing two subdirectorys '\n",
    "                             '`images/val2014` and `annotations`. Right now only val2014 '\n",
    "                             'dataset with person_keypoints are supported',\n",
    "                        type=str)\n",
    "    parser.add_argument('--representative-datapath',\n",
    "                        '-reprdata',\n",
    "                        help='The location where representative data are stored. '\n",
    "                             'The data will be used for computation of encodings',\n",
    "                        type=str)\n",
    "    parser.add_argument('--quant-scheme',\n",
    "                        '-qs',\n",
    "                        help='Support two schemes for quantization: [`tf` or `tf_enhanced`],'\n",
    "                             '`tf_enhanced` is used by default',\n",
    "                        default='tf_enhanced',\n",
    "                        choices=['tf', 'tf_enhanced'],\n",
    "                        type=str)\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    pose_estimation_quanteval(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中用到的资源链接如下：\n",
    "* 已经压缩过的pose_estimation神经网络模型：\n",
    "https://github.com/quic/aimet-model-zoo/releases/download/pose_estimation_pytorch/pose_estimation_pytorch_weights.tgz\n",
    "* COCO2014验证/测试数据集：\n",
    "http://images.cocodataset.org/zips/val2014.zip\n",
    "或者\n",
    "http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
    "* 具体内容参考：\n",
    "https://github.com/quic/aimet-model-zoo/blob/develop/zoo_torch/Docs/PoseEstimation.md\n",
    "\n",
    "准备好模型文件和数据集后，你可以合并以上Jupyter笔记本的Python代码片段到一个Python脚本中，用一下命令执行它：\n",
    "```\n",
    "python ./zoo_torch/examples/pose_estimation_quanteval.py ./network/Pose-Estimation/pe_weights.pth ./dataset/COCO-2014/\n",
    "```\n",
    "\n",
    "我们来看一下输出的结果：\n",
    "```\n",
    "$ python ./zoo_torch/examples/pose_estimation_quanteval.py ./network/Pose-Estimation/pe_weights.pth ./dataset/COCO-2014/\n",
    "2021-12-29 14:19:32,211 - root - INFO - AIMET\n",
    "defining network with shared weights\n",
    "2021-12-29 14:19:32,504 - Quant - INFO - No config file provided, defaulting to config file at /home/user_name/....../anaconda3/envs/env_name/lib/python3.7/site-packages/aimet_common/quantsim_config/default_config.json\n",
    "2021-12-29 14:19:32,512 - Utils - INFO - ...... subset to store [Conv_0, Relu_1]\n",
    "2021-12-29 14:19:32,512 - Utils - INFO - ...... subset to store [Conv_3, Relu_4]\n",
    "2021-12-29 14:19:32,512 - Utils - INFO - ...... subset to store [Conv_7, Relu_8]\n",
    "2021-12-29 14:19:32,512 - Utils - INFO - ...... subset to store [Conv_10, Relu_11]\n",
    "2021-12-29 14:19:32,512 - Utils - INFO - ...... subset to store [Conv_14, Relu_15]\n",
    "2021-12-29 14:19:32,512 - Utils - INFO - ...... subset to store [Conv_17, Relu_18]\n",
    "2021-12-29 14:19:32,512 - Utils - INFO - ...... subset to store [Conv_20, Relu_21]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_23, Relu_24]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_27, Relu_28]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_30, Relu_31]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_33, Relu_34]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_35, Relu_36]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_38, Relu_39]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_41, Relu_42]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_44, Relu_45]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_47, Relu_48]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_50, Relu_51]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_52, Relu_53]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_69, Relu_70]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_72, Relu_73]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_75, Relu_76]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_78, Relu_79]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_81, Relu_82]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_83, Relu_84]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_87, Relu_88]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_90, Relu_91]\n",
    "2021-12-29 14:19:32,513 - Utils - INFO - ...... subset to store [Conv_93, Relu_94]\n",
    "2021-12-29 14:19:32,514 - Utils - INFO - ...... subset to store [Conv_95, Relu_96]\n",
    "2021-12-29 14:19:32,514 - Utils - INFO - ...... subset to store [Conv_56, Relu_57]\n",
    "2021-12-29 14:19:32,514 - Utils - INFO - ...... subset to store [Conv_59, Relu_60]\n",
    "2021-12-29 14:19:32,514 - Utils - INFO - ...... subset to store [Conv_62, Relu_63]\n",
    "2021-12-29 14:19:32,514 - Utils - INFO - ...... subset to store [Conv_64, Relu_65]\n",
    "Initializing demo for *keypoints* results.\n",
    "loading annotations into memory...\n",
    "Done (t=1.44s)\n",
    "creating index...\n",
    "index created!\n",
    "Initializing demo for *keypoints* results.\n",
    "loading annotations into memory...\n",
    "Done (t=1.33s)\n",
    "creating index...\n",
    "index created!\n",
    "Running extended evaluation on the validation set\n",
    "500it [08:29,  1.02s/it]\n",
    "Initializing demo for *keypoints* results.\n",
    "loading annotations into memory...\n",
    "Done (t=1.41s)\n",
    "creating index...\n",
    "index created!\n",
    "Loading and preparing results...\n",
    "DONE (t=0.02s)\n",
    "creating index...\n",
    "index created!\n",
    "Initializing demo for *keypoints* results.\n",
    "loading annotations into memory...\n",
    "Done (t=1.34s)\n",
    "creating index...\n",
    "index created!\n",
    "Initializing demo for *keypoints* results.\n",
    "loading annotations into memory...\n",
    "Done (t=1.43s)\n",
    "creating index...\n",
    "index created!\n",
    "Running per image evaluation...\n",
    "Evaluate annotation type *keypoints*\n",
    "DONE (t=0.25s).\n",
    "Accumulating evaluation results...\n",
    "DONE (t=0.01s).\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.381\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.688\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.371\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.313\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.466\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.441\n",
    " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.714\n",
    " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.446\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.322\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.583\n",
    "Initializing demo for *keypoints* results.\n",
    "loading annotations into memory...\n",
    "Done (t=1.34s)\n",
    "creating index...\n",
    "index created!\n",
    "Initializing demo for *keypoints* results.\n",
    "loading annotations into memory...\n",
    "Done (t=1.44s)\n",
    "......\n",
    "creating index...\n",
    "index created!\n",
    "Running per image evaluation...\n",
    "Evaluate annotation type *keypoints*\n",
    "DONE (t=0.25s).\n",
    "Accumulating evaluation results...\n",
    "DONE (t=0.00s).\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.370\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.685\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.363\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.304\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.455\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.433\n",
    " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.712\n",
    " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.432\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.313\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.578\n",
    "The [mAP, mAR] results are: [0.36965444 0.43328   ]\n",
    "(aimet) user_name@user_name-(aimet) user_name@user_name-Ozoouser_name@user_name-Opti(aimet) user_name@user_name-OptiPlersoft-OptiPlex-7080:~/workpace(aimet) user_name@user_name-OptiPl-7080:~/workpace/dp/aimet-model-zoo$(aimet) user_name@user_name-OptiPlex-7080:~/workpace/dp/aimet-model-zoo$ (aimet) user_name@user_name-OptiPlex-7080:~/workpace/dp/aimet-model-zoo$ index created!results.\n",
    "loading annotations into memory...\n",
    "Done (t=1.44s)\n",
    "creating index...\n",
    "index created!\n",
    "Running extended evaluation on the validation set\n",
    "500it [11:50,  1.42s/it]\n",
    "Initializing demo for *keypoints* results.\n",
    "loading annotations into memory...\n",
    "Done (t=1.38s)\n",
    "......\n",
    "creating index...\n",
    "index created!\n",
    "Running per image evaluation...\n",
    "Evaluate annotation type *keypoints*\n",
    "DONE (t=0.25s).\n",
    "Accumulating evaluation results...\n",
    "DONE (t=0.00s).\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.370\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.685\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.363\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.304\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.455\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 20 ] = 0.433\n",
    " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets= 20 ] = 0.712\n",
    " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets= 20 ] = 0.432\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets= 20 ] = 0.313\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets= 20 ] = 0.578\n",
    "The [mAP, mAR] results are: [0.36965444 0.43328   ]\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7fc01009719acde1fe3da7930d50803a7884fcf39413b528a70d4062f91afcf"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('aimet': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
